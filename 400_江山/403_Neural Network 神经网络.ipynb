{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Neural Network 神经网络",
   "id": "41fe8b7b1d54893c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 0 常见的神经网络结构\n",
    "\n",
    "1. 卷积神经网络(CNN)\n",
    "\n",
    "   专门用于处理*图像*数据的深度学习模型，*通过卷积层提取特征，并通过池化层降低数据维度*。\n",
    "\n",
    "2. 循环神经网络(RNN)\n",
    "\n",
    "   适用于处理*序列*数据，如*时间序列和文本*，*通过内部状态的循环传递来捕捉时序依赖*。\n",
    "\n",
    "3. 生成对抗网络(GAN)\n",
    "\n",
    "   由`生成器`（学生）和`判别器`（老师）组成的网络，用于*生成新的数据样本*，广泛应用于*图像生成*（文生图）和*风格转换*。\n",
    "\n",
    "4. 变换器(Transformer)\n",
    "\n",
    "   基于`自注意力机制`的模型，用于处理*序列*数据，如自然语言处理中的*文本翻译和生成任务*。"
   ],
   "id": "be606e7dded52d36"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1 实战极简神经网络",
   "id": "7460466bed3c573c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T09:17:58.584810Z",
     "start_time": "2025-02-16T09:17:53.702140Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n"
   ],
   "id": "a3d8bac14c2c2b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2\n",
      "True\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.1 【原理版】自定义张量的线性回归神经网络模型",
   "id": "dc29b24dd7120950"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T09:17:58.710812Z",
     "start_time": "2025-02-16T09:17:58.596816Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 特征张量（输入）\n",
    "x = torch.tensor([\n",
    "    [1, 0, 0],\n",
    "    [1, 1, 0],\n",
    "    [1, 0, 1],\n",
    "    [1, 1, 1]\n",
    "], dtype=torch.float32)\n",
    "print(f\"特征张量x（输入）：\\n{x}\")\n",
    "\n",
    "# 模拟预测参数\n",
    "w = torch.rand(3, 1)\n",
    "print(f\"预测参数w：\\n{w}\")\n",
    "\n",
    "# 模拟预测张量（标签）\n",
    "y_hat = x @ w\n",
    "print(f\"预测张量y_hat（标签）：\\n{y_hat}\")\n"
   ],
   "id": "5f2e0a69b664f2b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征张量x（输入）：\n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 0., 1.],\n",
      "        [1., 1., 1.]])\n",
      "预测参数w：\n",
      "tensor([[0.7906],\n",
      "        [0.6718],\n",
      "        [0.1030]])\n",
      "预测张量y_hat（标签）：\n",
      "tensor([[0.7906],\n",
      "        [1.4624],\n",
      "        [0.8935],\n",
      "        [1.5653]])\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.2 【框架版】原生封装的线性回归神经网络模型",
   "id": "7815ad009767f0d7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T09:17:59.250447Z",
     "start_time": "2025-02-16T09:17:59.206447Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 特征张量（输入），去除偏置项\n",
    "x = torch.tensor([\n",
    "    [0, 0],\n",
    "    [1, 0],\n",
    "    [0, 1],\n",
    "    [1, 1]\n",
    "], dtype=torch.float32)\n",
    "print(f\"特征张量x（输入），去除偏置项：\\n{x}\")\n",
    "\n",
    "# Linear实例参数：\n",
    "# in_features 上一层神经元的个数（输入特征），out_features 下一层神经元的个数（输出特征）\n",
    "output = torch.nn.Linear(2, 1)\n",
    "print(f\"线性回归模型的预测张量：\\n{output(x)}\")\n",
    "print(f\"线性回归模型的参数：\\n{output.weight}\")\n",
    "print(f\"线性回归模型的偏置项：\\n{output.bias}\")\n"
   ],
   "id": "164602864995f14e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征张量x（输入），去除偏置项：\n",
      "tensor([[0., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 1.]])\n",
      "线性回归模型的预测张量：\n",
      "tensor([[-0.1320],\n",
      "        [-0.8349],\n",
      "        [-0.0852],\n",
      "        [-0.7881]], grad_fn=<AddmmBackward0>)\n",
      "线性回归模型的参数：\n",
      "Parameter containing:\n",
      "tensor([[-0.7029,  0.0468]], requires_grad=True)\n",
      "线性回归模型的偏置项：\n",
      "Parameter containing:\n",
      "tensor([-0.1320], requires_grad=True)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2 损失函数",
   "id": "f1bc78f59873be93"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "每一个样本经过模型后会得到一个预测值，然后得到的预测值和真实值的差值就成为损失（当然损失值越小证明模型越是成功），我们知道有许多不同种类的损失函数，这些函数本质上就是计算预测值和真实值的差距的一类型函数。",
   "id": "f6a72fa5c3398e42"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "很多损失函数的计算结果都与输入特征的范数的值相关。",
   "id": "aa876f30cc02ec72"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T09:17:59.298185Z",
     "start_time": "2025-02-16T09:17:59.285195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = torch.tensor([\n",
    "    [1, 2, 3, 4],\n",
    "    [2, 4, 6, 8],\n",
    "    [3, 6, 9, 12]\n",
    "], dtype=torch.float32)\n",
    "res_l2 = torch.norm(data, p=2, dim=1, keepdim=True)\n",
    "print(f\"输入特征：\\n{data}\")\n",
    "print(f\"L2范数：\\n{res_l2}\")\n"
   ],
   "id": "e8f4cbeba0140c47",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入特征：\n",
      "tensor([[ 1.,  2.,  3.,  4.],\n",
      "        [ 2.,  4.,  6.,  8.],\n",
      "        [ 3.,  6.,  9., 12.]])\n",
      "L2范数：\n",
      "tensor([[ 5.4772],\n",
      "        [10.9545],\n",
      "        [16.4317]])\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "常见的损失函数有：\n",
    "\n",
    "- 平均绝对误差（Mean Absolute Error, MAE），其中 $m$ 为样本数量（列数），$y_i$ 为真实值，$\\hat{y}_i$ 为预测值。\n",
    "    $$\\text{MAE} = L_1(y, \\hat{y}) = \\frac{1}{m} \\sum_{i=1}^m |y_i - \\hat{y}_i|$$\n",
    "- 均方误差（Mean Squared Error, MSE），用来衡量模型的预测值和真实值之间的*平均差异*。\n",
    "    $$\\text{MSE} = \\frac{1}{m} \\sum_{i=1}^m (y_i - \\hat{y}_i)^2$$\n",
    "- 交叉熵（Cross Entropy, CE），主要用于处理`分类`问题，用来衡量模型的预测值和真实值之间的*对数差异*。\n",
    "    $$\\text{CE} = -\\frac{1}{m} \\sum_{i=1}^m (y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i))$$\n",
    "    - `分类`任务中，预测值是一些`离散`的概率值（0或1）。特别地，若预测结果和真实情况`一致`，则`交叉熵为0`，否则交叉熵为正值。\n",
    "    - `回归`任务中，预测值是一些`连续`的概率值。"
   ],
   "id": "6acb5d32d3703eba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T09:17:59.362182Z",
     "start_time": "2025-02-16T09:17:59.323183Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y = torch.tensor([2, 3, 4, 5], dtype=torch.float32)\n",
    "y_hat = torch.tensor([4, 5, 6, 7], dtype=torch.float32)\n",
    "print(f\"真实值：\\n{y}\")\n",
    "print(f\"预测值：\\n{y_hat}\")\n",
    "\n",
    "# 继承基类nn，定义损失函数接口loss，并计算损失差异\n",
    "mae = torch.nn.L1Loss()\n",
    "print(f\"平均绝对误差MAE损失：\\n{mae(y, y_hat)}\")\n",
    "mse = torch.nn.MSELoss()\n",
    "print(f\"均方误差MSE损失：\\n{mse(y, y_hat)}\")\n",
    "ce = torch.nn.CrossEntropyLoss()\n",
    "print(f\"交叉熵CE损失：\\n{ce(y, y_hat)}\")\n"
   ],
   "id": "cd598c865340b31c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "真实值：\n",
      "tensor([2., 3., 4., 5.])\n",
      "预测值：\n",
      "tensor([4., 5., 6., 7.])\n",
      "平均绝对误差MAE损失：\n",
      "2.0\n",
      "均方误差MSE损失：\n",
      "4.0\n",
      "交叉熵CE损失：\n",
      "37.684173583984375\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3 最优解（局部极值与全局最值）与梯度下降【神经网络的核心命题】",
   "id": "d32008f97c2076c2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "目标函数最优解问题解法：\n",
    "\n",
    "1. 求导数法：\n",
    "   简单函数的最优解就是函数的导数为0的点。\n",
    "2. 梯度下降法【重点】（基于`循环选代`式优化方法）：\n",
    "   复杂函数的最优解就是函数的梯度方向的负方向。\n",
    "3. 随机梯度下降法：\n",
    "   随机梯度下降法是梯度下降法的一种改进版本，每次迭代时只选取一个样本来计算梯度，从而减少计算量。\n",
    "4. 随机梯度下降法（动量）：\n",
    "   随机梯度下降法的改进版本，在计算梯度时考虑之前的梯度，从而加速收敛速度。"
   ],
   "id": "f3128b2f668f5633"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "设函数 $f(x)$ ，求 $f(x)$ 的最小值点 $x$，可根据如下梯度下降法来求解最优解 $x$ 收敛于常数，其中 $\\eta$ 为学习率（步长）：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x_{n+1} &= x_n - \\eta \\nabla f(x_n) \\\\\n",
    "&= x_n - \\eta \\frac{\\partial f(x_n)}{\\partial x_n}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "学习率（Learning Rate）是机器学习和深度学习中的一个关键超参数，它决定了在梯度下降优化过程中模型参数更新的步长。一个合适的学习率可以帮助模型快速收敛到最优解，而不适当的学习率可能导致模型训练不稳定（振荡）、收敛速度慢或无法收敛到最优解。\n"
   ],
   "id": "1965687e9c11a764"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4 反向传播算法【神经网络的核心思想】",
   "id": "aa26b139b5176622"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "![前向传播.png](assets/imgs/前向传播.png)\n",
    "\n",
    "当神经网络完成了一次前向传播的全过程之后，要计算最终输出的预测值与实际值之间的损失，来更新权重，从而最小化损失差异。"
   ],
   "id": "da56a4e5c509aac3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "反向传播算法(Back Propagation)是一种用于训练多层神经网络的学习方法，它通过计算损失函数相对于网络权重的梯度来更新权重，以便最小化损失函数。\n",
    "\n",
    "反向传播算法的关键在于`链式法则`，它允许我们计算复杂函数的`梯度`（求导），即使这些函数是由多个简单函数复合而成的。在实际应角中，反向传播算法是训练深度学习模型的基石，广泛应用于图像识别、语音识别、自然语言处理等领域。"
   ],
   "id": "ae8b876cbe431881"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "![反向传播算法的基本步骤.png](assets/imgs/反向传播算法的基本步骤.png)\n",
    "\n",
    "迭代次数可依据对模型的要求自行制定，反向传播算法的迭代过程会在达到预设的迭代次数，或者损失小于某一阈值，即准确率达到预期目标时停止，此时神经网络收敛，能够成功训练出模型。"
   ],
   "id": "6a4f2363a59a48b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
